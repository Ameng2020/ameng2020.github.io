# Top-down学习法

1） **每个季度先定义一个“成品” （Product Definition）**

把所有学习都绑定到可交付物上：**不写学习X, 只写交付Y**

 把2026的两个主线成品（建议这样表述）

- **Agent系统开发**： 一个“可部署、可评测、可追踪、可使用”的安全运营/事件调查 Agent （含运行代码 + 回归测评 + 文档）
- **海外尝试：**一个英文作品集页面（含2个项目演示/说明） + 一份英文简历 + 一张机会追踪表 （持续更新）

> 后续要学习的Python 、Prompt、 Eval、 LangGraph、可观测性，都必须能回答： 它能让成品的哪一部分更好？



**2）把AI知识递归 变成 固定工作流 （Knowledge Recursion Loop）**

你不是“问AI写代码”，而是每次遇到卡点，强制做3问递归：

**递归三问（每次必问）**

1. 这段代码/ 方案背后的逻辑是什么？ （用流程图/步骤解释）
2. 为什么这里要这样设计（或用这个公式、指标）？替代方案是什么？trade-off？
3. 给我一个最小可跑例子 + 2个反例（会失败的输入），并告诉我怎么兜底。

**强制产物（每次递归后必须落地）**

- decision.md: 一句话决策 + 原因 + 备选方案 + 选型结论
- test_cases.md: 新增 1-3条回归用例（成功、失败各至少1条）
- kb_note.md : 一条知识库笔记（背景、结论、证据链接、下一步）

这样你问AI的结果不会蒸发，会变成“工程资产”

**3）建立“直觉” 而不是背知识点 （Intuition Building）**

把抽象概念变成可视化、可类比、可验证的直觉。

**你可以固定让AI用3种方式解释任何概念**

-  画图：给我一张图解释X在系统里的位置（输入 --> 处理 --> 输出）
- 类比：用一个生活类比解释X的作用（但要说明类比的边界）
- 验证：给我一个能证明它有效、无效的最小实验（怎么测、看什么指标）

**适用到你 Agent 项目里最关键的几类概念**

- “工具调用为什么会乱？ ” --> 让AI画：意图--> 参数抽取 --> 效验 --> 工具返回 --> 后处理 --> 记忆更新
- “评测指标怎么选？" ---> 让AI给：指标定义、可观测数据来源、容易被刷的漏洞



**4）把“快速失败、快速迭代” 写成硬性节奏 （Feedback Loop）**

Top-down 的精髓是反馈回路：每周必须有可见交付 + 必须有Bug/失败样本沉淀

**建议你写进计划里的赢规则**

- 每周至少1个可见交付物（PR、版本、可用用例、文档、演示脚本）
- 每周至少记录3个失败样本（模型误判、工具失败、参数缺失、结果不可信），并给出修复策略
- 每月跑一次回归评测，产出一份quality_report.md (哪怕很短)





AI递归提示词模版：

把下面当成你每次卡住时的标准问法：

模版：

- 背景：我在做【成品X】的【模块Y】
- 目标：我要实现【具体行为/接口/指标】
- 现状：我现在卡在【具体报错/不稳定表演/设计决策】
- 约束：必须满足【性能/安全/可观测/可维护】
- 你需要输出：
  1. 最小可跑实现逻辑
  2. 背后的逻辑分解（步骤、图）
  3. 2个替代方案 + trade-off
  4. 失败样本与兜底策略
  5. 我应该新增哪些回归用例



